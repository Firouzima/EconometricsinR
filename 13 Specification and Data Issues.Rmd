---
title: "13 Specification and Data Issues"
author: "Mohammadamin Firouzi"
date: "2025-05-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Specification and Data Issues

#### Misspecification of the Functional Form:

```{r message=FALSE, warning=FALSE}

set.seed(1)

x = runif(250, -10, 10)
y = x^2 + rnorm(250) 

model = lm(y ~ x)
summary(model)
cat('\n')

plot(x, y, main = 'Y vs X', pch = 20, col = '#029143')
abline(model, col = 'steelblue', lwd = 2)

qmodel = lm(y ~ x + I(x^2))
summary(qmodel)
```

#### The Regression Equation Specification Error Test (RESET):

```{r message=FALSE, warning=FALSE}

resetmodel = lm(y ~ x + I(fitted(model)^2) + I(fitted(model)^3))

library(car)
linearHypothesis(resetmodel, matchCoefs(resetmodel, 'fitted')) 
# null hypothesis: this equation is correctly specified
cat('\n')

library(lmtest)
resettest(model)
resettest(qmodel)
```

#### Measurement Error:

Load the package 'mvtnorm' and simulate bivariate normal data.

$$
(X, Y) \sim \mathcal{N} \left( \begin{pmatrix}50 \\ 100\end{pmatrix},\begin{pmatrix}10 & 5 \\ 5 & 10\end{pmatrix}\right)
$$

Population regression function:

$$
Y_i = 100 + 0.5(X_i - 50)= 75 + 0.5X_i
$$

```{r message=FALSE, warning=FALSE}

library(mvtnorm)

# Measurment error in the indep. var.:

set.seed(123)

data = data.frame(rmvnorm(1000, c(50, 100),
                  sigma = cbind(c(10, 5), c(5, 10))))
colnames(data) = c('x', 'y')

without_error_model = lm(y ~ x, data = data)
summary(without_error_model)

data$x = data$x + rnorm(1000, sd = sqrt(10))

error_model = lm(y ~ x, data = data)
summary(error_model)

plot(data$x, data$y, pch = 20, col = '#897682', xlab = 'x', ylab = 'y')
abline(without_error_model, col = 'green', lwd = 2)
abline(error_model, col = 'darkblue', lwd = 2)
abline(a = 75, b = 0.5, col = 'black', lwd = 2)
legend('topleft', bg = 'transparent',
       col = c('green', 'darkblue', 'black'),
       legend = c('Without Error', 'With Error', 'Population'),
       lwd = 2, bty = 2)


```

In the situation without measurement error, the estimated regression function is close to the population regression function. Things are different when we use the mismeasured regressor X: both the estimate for the intercept and the estimate for the coefficient on X differ considerably from results obtained using the “clean” data on X.

```{r}

# Measurement error in the dep. var.:

set.seed(1)

alpha = 1; beta = 5

betahat = numeric(10000)
betahat.me = numeric(10000)

x = rnorm(1000, 4, 1)

for (j in 1:10000) {
  u = rnorm(1000)
  ystar = alpha + beta*x + u
  bhat = coef(lm(ystar ~ x))
  betahat[j] = bhat['x']
  e0 = rnorm(1000)
  y = ystar + e0
  bhat.me = coef(lm(y ~ x))
  betahat.me[j] = bhat.me['x']
}

# Mean with and without ME:
c(mean(betahat), mean(betahat.me))
# Variance with and without ME:
c(var(betahat), var(betahat.me))

```

In the simulation, the parameter estimates using both the correct y and the mismeasured y are stored as the variables **betahat** and **betahat.me**, respectively. As expected, the simulated mean of both variables is close to the expected value of $\beta_1$ = 5. The variance of **betahat.me** is around 0.0019 which is twice as high as the variance of **betahat**. This was expected since in our simulation, u and e0 are both independent standard normal variables, so Var(u) = 1 and Var(u + e0) = 2.

#### Missing Data and Sample Selection:

```{r message=FALSE, warning=FALSE}

library('wooldridge')
data('card')
attach(card)

fatheduc = card$fatheduc
missfed = is.na(fatheduc)

rbind(fatheduc, missfed)[, 500:510]
```

```{r message=FALSE, warning=FALSE}

table(missfed)
```

```{r message=FALSE, warning=FALSE}

colSums(is.na(card))
```

```{r message=FALSE, warning=FALSE}

c = complete.cases(card)
table(c)
```

The function complete.cases(mydata) generates one logical vector indicating the rows of the dataframe that don’t have any missing information.

#### The three cases are:

1\. Data are missing at random.

```{r message=FALSE, warning=FALSE}

set.seed(563)

x = rnorm(1000, 5, 0.1)
y = 3 + 5 * x + rnorm(1000)

dat = data.frame(y, x)

smpl = sample(1:1000, size = 500)

plot(dat$x[-smpl], dat$y[-smpl], col = 'green', pch = 20,
     cex = 1, xlab = 'X', ylab = 'Y')
points(dat$x[smpl], dat$y[smpl], col = 'cyan', pch = 20,
       cex = 1)
abline(coef = c(3, 5), col = 'blue', lwd = 2)

full_smpl = lm(y ~ x, data = dat)
abline(full_smpl, col = 'red', lwd = 2)

half_smpl = lm(y ~ x, data = dat[-smpl,])
abline(half_smpl, col = 'orange')

legend('topleft', lty = 1, bg = 'transparent', cex = 0.8,
       col = c('blue', 'red', 'orange'), legend = c('Population', 'Full Sample', '500 obs. randomly selected'))
```

2\. Data are missing based on the value of a regressor.

```{r message=FALSE, warning=FALSE}

x_under_five = dat$x < 5

plot(dat$x[-x_under_five], dat$y[-x_under_five], col = '#82493f',
     pch = 20, xlab = 'X', ylab = 'Y', main = 'Y vs. X')
points(dat$x[x_under_five], dat$y[x_under_five], col = '#93703a', 
       pch = 20, cex = 1)

abline(a = 3, b = 5, col = 'blue', lwd = 2)
abline(full_smpl, col = 'red', lwd = 2)

x_under_five = which(dat$x < 5)

under_five_reg = lm(y ~ x, data = dat[x_under_five, ])

abline(under_five_reg, col = 'orange', lwd = 2)
```

3\. Data are missing due to a selection process which is related to the dependent variable.

```{r message=FALSE, warning=FALSE}

id = which(dat$x < 5.2 & dat$y < 28)

plot(dat$x[-id], dat$y[-id], col = '#82493f', lwd = 2, cex = 1,
     pch = 20, xlab = 'X', ylab = 'Y', main = 'Y vs. X', ylim = c(25, 31.5))
points(dat$x[id], dat$y[id], col = '#93703a', lwd = 2, cex = 1, pch = 20)

abline(coef = c(3, 5), lwd = 2, col = 'blue')
abline(full_smpl, lwd = 2, col = 'red')

smpl_bias_sel_reg = lm(y ~ x, data = dat[id, ])

abline(smpl_bias_sel_reg, lwd = 2, col = 'orange')
```
